{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers , optimizers , datasets ,Sequential ,metrics\n(x, y) , (x_test , y_test) = datasets.mnist.load_data()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x, y):\n    x = tf.cast(x , dtype= tf.float32) /255.\n    y = tf.cast(y,dtype=tf.int32)\n    return x, y","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsz = 128","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(10000).batch(batchsz)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\ndb_test = db_test.map(preprocess).batch(batch_size=batchsz)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_iter = iter(db)\nsample = next(db_iter)\nprint('batch:' , sample[0].shape ,sample[1].shape)","execution_count":22,"outputs":[{"output_type":"stream","text":"batch: (128, 28, 28) (128,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    layers.Dense(256, tf.nn.relu),\n    layers.Dense(128, tf.nn.relu),\n    layers.Dense(64, tf.nn.relu),\n    layers.Dense(32, tf.nn.relu),\n    layers.Dense(10)   \n])\nmodel.build(input_shape=[None ,28*28])\nmodel.summary()","execution_count":23,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 256)               200960    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 244,522\nTrainable params: 244,522\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optimizers.Adam(lr=1e-3)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(30):\n    for step, (x, y) in enumerate(db):\n        x = tf.reshape(x, [-1, 28 * 28])\n        with tf.GradientTape() as tape:\n            logits = model(x)\n            y_onehot = tf.one_hot(y, depth=10)\n            loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n            loss_ce = tf.losses.categorical_crossentropy(y_onehot,\n                                                         logits,\n                                                         from_logits=True)\n            loss_ce = tf.reduce_mean(loss_ce)\n\n        grads = tape.gradient(loss_ce, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n\n            #             #test\n    total_correct = 0\n    total_num = 0\n    for x, y in db_test:\n        x = tf.reshape(x, [-1, 28 * 28])\n        logits = model(x)\n        prob = tf.nn.softmax(logits, axis=1)\n        pred = tf.argmax(prob, axis=1)\n        pred = tf.cast(pred, dtype=tf.int32)\n        correct = tf.equal(pred, y)\n        correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n\n        total_correct += int(correct)\n        total_num += x.shape[0]\n    acc = total_correct / total_num    \n    print(epoch,'test acc' ,acc)\n    ","execution_count":null,"outputs":[{"output_type":"stream","text":"0 0 loss: 0.03167692571878433 29.402713775634766\n0 100 loss: 0.019802510738372803 36.26633834838867\n0 200 loss: 0.04704906791448593 37.99005126953125\n0 300 loss: 0.022478271275758743 32.15018844604492\n0 400 loss: 0.11422568559646606 35.02384948730469\n0 test acc 0.9737\n1 0 loss: 0.031096648424863815 29.94021987915039\n1 100 loss: 0.008172761648893356 38.496604919433594\n1 200 loss: 0.0023693502880632877 41.98213195800781\n1 300 loss: 0.007467607501894236 42.995140075683594\n1 400 loss: 0.010752130299806595 38.128326416015625\n1 test acc 0.9767\n2 0 loss: 0.05066175386309624 32.60882568359375\n2 100 loss: 0.007179488427937031 43.025115966796875\n2 200 loss: 0.022776931524276733 53.033424377441406\n2 300 loss: 0.10913066565990448 47.601478576660156\n2 400 loss: 0.010676905512809753 52.85319519042969\n2 test acc 0.9794\n3 0 loss: 0.04869420453906059 48.71522903442383\n3 100 loss: 0.016710542142391205 45.26734161376953\n3 200 loss: 0.004624741617590189 47.733802795410156\n3 300 loss: 0.012886697426438332 48.65544128417969\n3 400 loss: 0.004854439292103052 48.516845703125\n3 test acc 0.9793\n4 0 loss: 0.02462667226791382 42.928321838378906\n4 100 loss: 0.006265551783144474 46.909339904785156\n4 200 loss: 0.009048028849065304 51.36690902709961\n4 300 loss: 0.05324721708893776 52.5609130859375\n4 400 loss: 0.0003667819546535611 64.14171600341797\n4 test acc 0.9797\n5 0 loss: 0.011829863302409649 57.49214172363281\n5 100 loss: 0.014671731740236282 61.422882080078125\n5 200 loss: 0.005228462629020214 53.694862365722656\n5 300 loss: 0.0012779886601492763 56.28891372680664\n5 400 loss: 0.0010001418413594365 59.293724060058594\n5 test acc 0.9755\n6 0 loss: 0.02759765274822712 63.766868591308594\n6 100 loss: 0.003272268921136856 54.940086364746094\n6 200 loss: 0.019480861723423004 56.57079315185547\n6 300 loss: 0.000583315035328269 66.95721435546875\n6 400 loss: 0.006473612505942583 64.81259155273438\n6 test acc 0.9793\n7 0 loss: 0.03708914667367935 63.38037109375\n7 100 loss: 0.00720318453386426 70.23838806152344\n7 200 loss: 0.00453380448743701 62.29412078857422\n7 300 loss: 0.03962705656886101 61.897911071777344\n7 400 loss: 0.010411459021270275 68.94186401367188\n7 test acc 0.9793\n8 0 loss: 0.008184223435819149 73.4254379272461\n8 100 loss: 0.011576029472053051 72.16413116455078\n8 200 loss: 0.010411564260721207 63.79448699951172\n8 300 loss: 0.032275114208459854 64.20977783203125\n8 400 loss: 0.024687308818101883 56.03144836425781\n8 test acc 0.9802\n9 0 loss: 0.007447442039847374 60.7922477722168\n9 100 loss: 0.00032138184178620577 81.71000671386719\n9 200 loss: 0.0007642143173143268 71.37068176269531\n9 300 loss: 0.005759038496762514 80.81804656982422\n9 400 loss: 0.005335196387022734 77.66731262207031\n9 test acc 0.9802\n10 0 loss: 0.008546991273760796 74.93537902832031\n10 100 loss: 0.0034740574192255735 76.79071044921875\n10 200 loss: 0.0006410966743715107 75.40185546875\n10 300 loss: 0.017928671091794968 73.58195495605469\n10 400 loss: 0.023186108097434044 89.23955535888672\n10 test acc 0.9785\n11 0 loss: 0.00383085198700428 79.31123352050781\n11 100 loss: 0.01861291192471981 84.93156433105469\n11 200 loss: 0.006767749786376953 77.52437591552734\n11 300 loss: 0.0007584999548271298 74.0298080444336\n11 400 loss: 0.00041624222649261355 80.77400970458984\n11 test acc 0.9799\n12 0 loss: 0.005659305956214666 77.58900451660156\n12 100 loss: 0.0009733104379847646 89.33383178710938\n12 200 loss: 0.0018473509699106216 70.43406677246094\n12 300 loss: 0.0008398197242058814 81.07070922851562\n12 400 loss: 0.0003357511304784566 77.07411193847656\n12 test acc 0.9779\n13 0 loss: 0.003248838474974036 74.53373718261719\n13 100 loss: 0.02383626624941826 77.55740356445312\n13 200 loss: 0.0009799536783248186 70.08911895751953\n13 300 loss: 0.0017703062621876597 79.85769653320312\n13 400 loss: 0.00010506002581678331 89.7813491821289\n13 test acc 0.9787\n14 0 loss: 0.0018915326800197363 78.10543823242188\n14 100 loss: 0.0017361557111144066 81.30443572998047\n14 200 loss: 0.01144903339445591 80.78865814208984\n14 300 loss: 0.0007982662646099925 81.92695617675781\n14 400 loss: 0.036314088851213455 88.22369384765625\n14 test acc 0.9815\n15 0 loss: 0.00019324684399180114 93.61740112304688\n15 100 loss: 0.0004844500799663365 78.50679016113281\n15 200 loss: 0.00047050468856468797 75.95767211914062\n15 300 loss: 0.011519462801516056 85.38485717773438\n15 400 loss: 0.00760260596871376 88.01738739013672\n15 test acc 0.9807\n16 0 loss: 0.006577959284186363 74.92086029052734\n16 100 loss: 0.00011143318261019886 105.13382720947266\n16 200 loss: 0.01913677714765072 84.33029174804688\n16 300 loss: 0.11717798560857773 87.28424835205078\n16 400 loss: 0.07226172089576721 81.36885833740234\n16 test acc 0.98\n17 0 loss: 0.06146523728966713 75.74913024902344\n17 100 loss: 0.003071604762226343 77.2059326171875\n17 200 loss: 0.002360003534704447 82.92112731933594\n17 300 loss: 0.0019064286025241017 85.90158081054688\n17 400 loss: 0.02227988839149475 94.82029724121094\n17 test acc 0.9797\n18 0 loss: 0.013677596114575863 88.66439819335938\n18 100 loss: 0.0005471764598041773 111.12523651123047\n18 200 loss: 0.014480464160442352 85.07889556884766\n18 300 loss: 0.00026225639157928526 83.3718490600586\n18 400 loss: 0.012578417547047138 85.29166412353516\n18 test acc 0.9818\n19 0 loss: 0.020066214725375175 99.0618896484375\n19 100 loss: 0.0027903260197490454 82.56285858154297\n19 200 loss: 0.0001071629230864346 83.98363494873047\n19 300 loss: 0.011155425570905209 92.30702209472656\n19 400 loss: 0.0025521263014525175 92.57478332519531\n19 test acc 0.9807\n20 0 loss: 0.0008095003431662917 84.19790649414062\n20 100 loss: 0.0014912922633811831 94.0118408203125\n20 200 loss: 0.04965273663401604 79.31227111816406\n20 300 loss: 0.028755970299243927 89.24309539794922\n20 400 loss: 0.031171686947345734 92.7669677734375\n20 test acc 0.9825\n21 0 loss: 0.0024781276006251574 95.72323608398438\n21 100 loss: 0.00030465656891465187 82.82234954833984\n21 200 loss: 0.0020122339483350515 87.80934143066406\n21 300 loss: 0.03371323645114899 90.22886657714844\n21 400 loss: 0.02590591087937355 97.47344207763672\n21 test acc 0.9811\n22 0 loss: 0.001411156146787107 101.0295181274414\n22 100 loss: 0.00011300535697955638 106.94966125488281\n22 200 loss: 0.02413903921842575 100.55409240722656\n22 300 loss: 0.03629682585597038 88.27352905273438\n22 400 loss: 6.254798790905625e-05 83.89970397949219\n22 test acc 0.9809\n23 0 loss: 0.00011084173456765711 96.39350128173828\n23 100 loss: 0.002147073857486248 95.97526550292969\n23 200 loss: 0.0017681256867945194 106.6573486328125\n23 300 loss: 0.00013496619067154825 105.3128662109375\n23 400 loss: 0.0023385367821902037 98.42792510986328\n23 test acc 0.9818\n24 0 loss: 0.0006355494842864573 83.33802795410156\n24 100 loss: 0.0008448134176433086 90.91548919677734\n24 200 loss: 0.0010401769541203976 94.61405181884766\n24 300 loss: 0.0009624498197808862 76.06068420410156\n24 400 loss: 0.0011398684000596404 99.08035278320312\n24 test acc 0.9829\n25 0 loss: 2.4711327569093555e-05 102.75475311279297\n25 100 loss: 0.0010746835032477975 106.90196228027344\n25 200 loss: 0.001163018518127501 109.110595703125\n25 300 loss: 0.00018298489158041775 101.89983367919922\n25 400 loss: 0.009865461848676205 116.68994140625\n25 test acc 0.979\n26 0 loss: 0.10403919219970703 110.59220123291016\n26 100 loss: 1.917392000905238e-05 110.06990051269531\n26 200 loss: 0.00033298489870503545 84.54913330078125\n26 300 loss: 0.002430468099191785 96.0989990234375\n26 400 loss: 0.0006584554212167859 103.48165893554688\n26 test acc 0.9829\n27 0 loss: 0.0008015771745704114 106.02250671386719\n27 100 loss: 0.0021262350492179394 113.2020263671875\n27 200 loss: 0.04327277094125748 95.55812072753906\n27 300 loss: 0.0005615775007754564 104.32302856445312\n27 400 loss: 0.00017298461170867085 113.87493133544922\n27 test acc 0.9816\n28 0 loss: 0.0007333517423830926 120.22000885009766\n28 100 loss: 0.009122105315327644 113.28627014160156\n28 200 loss: 0.022178426384925842 84.37138366699219\n28 300 loss: 0.0003003353194799274 99.50895690917969\n28 400 loss: 0.0063736471347510815 97.68745422363281\n28 test acc 0.9805\n29 0 loss: 0.024799959734082222 116.91238403320312\n29 100 loss: 0.00472567044198513 109.30388641357422\n29 200 loss: 0.04463668167591095 114.97929382324219\n29 300 loss: 0.0008772799046710134 100.085693359375\n","name":"stdout"},{"output_type":"stream","text":"29 400 loss: 0.00012383046851027757 109.33740234375\n29 test acc 0.9805\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}